{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ, pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pydeequ import Check,CheckLevel\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.analyzers import *\n",
    "from pyspark.sql.functions import lit,current_timestamp\n",
    "from configparser import ConfigParser\n",
    "import sys\n",
    "import calendar,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Shivam\\\\Desktop\\\\training\\\\godaddy\\\\godaddy\\\\config\\\\config_analysis.json']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parser = ConfigParser()\n",
    "configFile = r\"C:\\Users\\Shivam\\Desktop\\training\\godaddy\\godaddy\\config\\config_analysis.json\"\n",
    "Parser.read(configFile)\n",
    "#Env = \"merchants\"\n",
    "#SourceFileName = \"merchants.csv\"\n",
    "\n",
    "##Set Job Parameters\n",
    "#Raw_S3_Path = Parser.get(Env, 'raw_s3_path')\n",
    "#Clean_S3_Path = Parser.get(Env, 'clean_s3_path')\n",
    "#Format = Parser.get(Env, 'file_format')\n",
    "#print(\"S3_Path = \" + Raw_S3_Path)\n",
    "#print(\"Format = \" + Format)\n",
    "#print(\"SourceFile = \" + S3_Path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEFAULT', 'merchants', 'users']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Parser.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_source', 'raw_s3_path', 'file_format', 'checks']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Parser['merchants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allAnalyzers(source_table):\n",
    "    analyzers = Parser.get(source_table,'checks').split(',')\n",
    "    return analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Completeness('id')\",\n",
       "  \" Distinctness('id')\",\n",
       "  \" Minimum('id')\",\n",
       "  \" Maximum('id')\",\n",
       "  \" Completeness('telephone')\"],\n",
       " [\"Completeness('id')\",\n",
       "  \" Completeness('email')\",\n",
       "  \" Minimum('sign_in_count')\",\n",
       "  \" Maximum('sign_in_count')\"]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(allAnalyzers,list(Parser.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"C:\\Program Files (x86)\\PostgreSQL\\pgJDBC\\postgresql-42.2.18.jar\") \\\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchants = spark.read.format('csv').load(Parser.get('merchants','raw_s3_path'),header=True,inferSchema=True).select('id','state','updated_at','telephone')\n",
    "df_users = spark.read.format('csv').load(Parser.get('users','raw_s3_path'),header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analysisResult = AnalysisRunner(spark)                     .onData(df)                     .addAnalyzer(eval(\"Size()\"))\\n\\nres = lambda fun : analysisResult.addAnalyzer(eval(fun))\\n\\nanalysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\\nanalysisResult_df.show() \\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "res = lambda fun : analysisResult.addAnalyzer(eval(fun))\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\n",
    "analysisResult_df.show() \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Result(analyzers,df, source_table):\n",
    "    analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "    res = lambda fun : analysisResult.addAnalyzer(eval(fun))\n",
    "\n",
    "    analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\n",
    "    analysisResult_df = analysisResult_df.withColumn('ingestion_tsp',lit(current_timestamp()))\n",
    "    analysisResult_df = analysisResult_df.withColumn('source_table',lit(source_table))\n",
    "    analysisResult_df.write.csv(f'..//analyzer_files//{source_table}_{calendar.timegm(time.gmtime())}',header=True,mode='overwrite')\n",
    "\n",
    "    analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"list_analyzers = list(map(allAnalyzers,list(Parser.keys())[1:]))\\n#list(map(Result,list_analyzers,['df_merchants','df_users']))\\nResult(list_analyzers[1],df_users)\\n\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''list_analyzers = list(map(allAnalyzers,list(Parser.keys())[1:]))\n",
    "#list(map(Result,list_analyzers,['df_merchants','df_users']))\n",
    "Result(list_analyzers[1],df_users)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "| entity| instance|        name|   value|       ingestion_tsp|source_table|\n",
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "|Dataset|        *|        Size|  1000.0|2020-12-22 22:55:...|   merchants|\n",
      "| Column|       id|Distinctness|     1.0|2020-12-22 22:55:...|   merchants|\n",
      "| Column|       id|Completeness|     1.0|2020-12-22 22:55:...|   merchants|\n",
      "| Column|       id|     Minimum|   469.0|2020-12-22 22:55:...|   merchants|\n",
      "| Column|       id|     Maximum|174577.0|2020-12-22 22:55:...|   merchants|\n",
      "| Column|telephone|Completeness|   0.999|2020-12-22 22:55:...|   merchants|\n",
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "\n",
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "| entity|     instance|        name|    value|       ingestion_tsp|source_table|\n",
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "| Column|sign_in_count|     Minimum|      0.0|2020-12-22 22:55:...|       users|\n",
      "| Column|sign_in_count|     Maximum|4187003.0|2020-12-22 22:55:...|       users|\n",
      "| Column|        email|Completeness|      1.0|2020-12-22 22:55:...|       users|\n",
      "|Dataset|            *|        Size| 154817.0|2020-12-22 22:55:...|       users|\n",
      "| Column|           id|Completeness|      1.0|2020-12-22 22:55:...|       users|\n",
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(*map(Result,list_analyzers,[df_merchants,df_users],list(Parser.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysisResult = AnalysisRunner(spark) \\\n",
    " #                   .onData(df) \\\n",
    "  #                  .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "#for fun in analyzers:\n",
    " #   analysisResult = analysisResult.addAnalyzer(eval(fun))\n",
    "#analysisResult = analysisResult.run()\n",
    "  #.addAnalyzer(CountDistinct(\"state\")) \\\n",
    "                  \n",
    "                   \n",
    "\n",
    "#analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "#analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
