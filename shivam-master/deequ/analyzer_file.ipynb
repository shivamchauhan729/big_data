{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pydeequ import Check,CheckLevel\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.analyzers import *\n",
    "from pyspark.sql.functions import lit,current_timestamp\n",
    "from configparser import ConfigParser\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_Path = C:\\\\Users\\\\Shivam\\\\Desktop\\\\training\\\\godaddy\\\\godaddy\\\\data\\\\merchants.csv\n",
      "Format = csv\n"
     ]
    }
   ],
   "source": [
    "Parser = ConfigParser()\n",
    "configFile = r\"C:\\Users\\Shivam\\Desktop\\training\\godaddy\\godaddy\\config\\config_analysis.json\"\n",
    "Parser.read(configFile)\n",
    "Env = \"dev\"\n",
    "#SourceFileName = \"merchants.csv\"\n",
    "\n",
    "##Set Job Parameters\n",
    "Raw_S3_Path = Parser.get(Env, 'raw_s3_path')\n",
    "#Clean_S3_Path = Parser.get(Env, 'clean_s3_path')\n",
    "Format = Parser.get(Env, 'file_format')\n",
    "print(\"S3_Path = \" + Raw_S3_Path)\n",
    "print(\"Format = \" + Format)\n",
    "#print(\"SourceFile = \" + S3_Path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Completeness('id')\",\n",
       " \" Distinctness('id')\",\n",
       " \" Minimum('id')\",\n",
       " \" Maximum('id')\",\n",
       " \" Completeness('telephone')\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzers = Parser.get(Env,'checks').split(',')\n",
    "analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"C:\\Program Files (x86)\\PostgreSQL\\pgJDBC\\postgresql-42.2.18.jar\") \\\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(Format).load(Raw_S3_Path,header=True,inferSchema=True).select('id','state','updated_at','telephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------------------+--------------+\n",
      "|    id|      state|          updated_at|     telephone|\n",
      "+------+-----------+--------------------+--------------+\n",
      "| 12498|trial_ended|2018-12-19 23:43:...|   45434323212|\n",
      "| 14319|trial_ended|2018-12-19 23:43:...|  413-478-8291|\n",
      "|  8338|trial_ended|2018-12-19 23:44:...|   09701189483|\n",
      "|130989|trial_ended|2020-08-03 17:01:...|00447921706558|\n",
      "| 59774|  suspended|2019-07-07 13:07:...|  000-000-0000|\n",
      "|  8284|trial_ended|2018-12-19 23:42:...|    2065126641|\n",
      "|158778|trial_ended|2020-11-03 01:21:...|    9783376017|\n",
      "|151006|  suspended|2020-10-12 17:12:...|  000-000-0000|\n",
      "|141731|  suspended|2020-11-30 12:26:...|  000-000-0000|\n",
      "| 50768|  suspended|2019-05-15 00:20:...|    7705476904|\n",
      "| 36169|trial_ended|2018-12-19 23:45:...|    4243039800|\n",
      "| 78396|  suspended|2020-03-01 02:13:...|    8018002701|\n",
      "| 45140|  pre_trial|2019-02-11 20:39:...|    9173281930|\n",
      "| 28378|trial_ended|2018-12-19 23:42:...|    5166038093|\n",
      "|123728|provisioned|2020-12-02 20:06:...|    8328233662|\n",
      "| 72067|  suspended|2019-11-11 12:45:...|  000-000-0000|\n",
      "| 12487|trial_ended|2018-12-19 23:43:...|   15612393009|\n",
      "|112280|trial_ended|2020-06-16 11:15:...|     029358092|\n",
      "| 98432|  pre_trial|2020-03-04 10:57:...|    9873956116|\n",
      "| 36039|  pre_trial|2018-12-19 23:44:...| +252615718145|\n",
      "+------+-----------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+--------+\n",
      "| entity| instance|        name|   value|\n",
      "+-------+---------+------------+--------+\n",
      "|Dataset|        *|        Size|  1000.0|\n",
      "| Column|       id|Distinctness|     1.0|\n",
      "| Column|       id|Completeness|     1.0|\n",
      "| Column|       id|     Minimum|   469.0|\n",
      "| Column|       id|     Maximum|174577.0|\n",
      "| Column|telephone|Completeness|   0.999|\n",
      "+-------+---------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "res = lambda fun : analysisResult.addAnalyzer(eval(fun))\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysisResult = AnalysisRunner(spark) \\\n",
    " #                   .onData(df) \\\n",
    "  #                  .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "#for fun in analyzers:\n",
    " #   analysisResult = analysisResult.addAnalyzer(eval(fun))\n",
    "#analysisResult = analysisResult.run()\n",
    "  #.addAnalyzer(CountDistinct(\"state\")) \\\n",
    "                  \n",
    "                   \n",
    "\n",
    "#analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "#analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult_df = analysisResult_df.withColumn('ingestion_tsp',lit(current_timestamp()))\n",
    "analysisResult_df = analysisResult_df.withColumn('source_table',lit(Parser.get(Env,'data_source')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "| entity| instance|        name|   value|       ingestion_tsp|source_table|\n",
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "|Dataset|        *|        Size|  1000.0|2020-12-21 16:55:...|   merchants|\n",
      "| Column|       id|Distinctness|     1.0|2020-12-21 16:55:...|   merchants|\n",
      "| Column|       id|Completeness|     1.0|2020-12-21 16:55:...|   merchants|\n",
      "| Column|       id|     Minimum|   469.0|2020-12-21 16:55:...|   merchants|\n",
      "| Column|       id|     Maximum|174577.0|2020-12-21 16:55:...|   merchants|\n",
      "| Column|telephone|Completeness|   0.999|2020-12-21 16:55:...|   merchants|\n",
      "+-------+---------+------------+--------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.jdbc(url=\"jdbc:postgresql://localhost:5433/postgres\",table='public.users_table',mode='overwrite',properties={\"user\": \"postgres\", \"password\": \"root\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult_df.write.csv('..//analyzer_files//merchant_analyzer',header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
