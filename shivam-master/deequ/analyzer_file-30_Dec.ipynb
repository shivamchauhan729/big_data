{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import pydeequ, pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pydeequ import Check,CheckLevel\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.analyzers import *\n",
    "from pyspark.sql.functions import lit,current_timestamp\n",
    "from configparser import ConfigParser\n",
    "import sys\n",
    "import calendar,time\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\config\\\\config_analysis_new.json']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parser = ConfigParser()\n",
    "configFile = r\"..\\config\\config_analysis_new.json\"\n",
    "Parser.read(configFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"C:\\Program Files (x86)\\PostgreSQL\\pgJDBC\\postgresql-42.2.18.jar\") \\\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read(sections):\n",
    "    if Parser.get(sections,\"file_format\")==\"csv\":\n",
    "        return(spark.read.format('csv').options(header=True,inferSchema=True).load(Parser.get(sections, 'raw_s3_path')))\n",
    "    elif Parser.get(sections,\"file_format\")==\"parquet\":\n",
    "        return(spark.read.parquet(Parser.get(sections,'raw_s3_path')).select('order_id','ordered_at_ts_utc','created_at_ts_utc','order_item_count','cancelled_at_ts_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Result(df, source_table):\n",
    "    analyzers = Parser.get(source_table,'metrics').split('|')\n",
    "    analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "    res = lambda fun : analysisResult.addAnalyzer(eval(fun))\n",
    "\n",
    "    analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\n",
    "    analysisResult_df = analysisResult_df.withColumn('ingestion_tsp',lit(current_timestamp()))\n",
    "    analysisResult_df = analysisResult_df.withColumn('source_table',lit(source_table))\n",
    "    analysisResult_df.write.csv(f'..//analyzer_files//{source_table}_{calendar.timegm(time.gmtime())}',header=True,mode='overwrite')\n",
    "\n",
    "    return analysisResult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+------------+--------------------+-----------------------+------------+\n",
      "|entity |instance                           |name        |value               |ingestion_tsp          |source_table|\n",
      "+-------+-----------------------------------+------------+--------------------+-----------------------+------------+\n",
      "|Dataset|*                                  |Size        |1000.0              |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |id                                 |Distinctness|1.0                 |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |id                                 |Completeness|1.0                 |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |id                                 |Minimum     |469.0               |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |id                                 |Maximum     |174577.0            |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |telephone                          |Completeness|0.999               |2020-12-30 19:53:42.777|merchants   |\n",
      "|Column |sign_in_count                      |Minimum     |0.0                 |2020-12-30 19:53:42.777|users       |\n",
      "|Column |sign_in_count                      |Maximum     |4187003.0           |2020-12-30 19:53:42.777|users       |\n",
      "|Column |email                              |Completeness|1.0                 |2020-12-30 19:53:42.777|users       |\n",
      "|Dataset|*                                  |Size        |154817.0            |2020-12-30 19:53:42.777|users       |\n",
      "|Column |id                                 |Completeness|1.0                 |2020-12-30 19:53:42.777|users       |\n",
      "|Column |order_id                           |Completeness|1.0                 |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |items_count more than 1            |Compliance  |0.05719101564031202 |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |order_id                           |Distinctness|1.0                 |2020-12-30 19:53:42.777|orders      |\n",
      "|Dataset|*                                  |Size        |51022.0             |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |cancelDate greater than createdDate|Compliance  |0.023578064364391832|2020-12-30 19:53:42.777|orders      |\n",
      "|Column |order_item_count                   |Completeness|0.9409274430637764  |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |order_item_count                   |Minimum     |1.0                 |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |order_item_count                   |Maximum     |53.0                |2020-12-30 19:53:42.777|orders      |\n",
      "|Column |orderDate greater than createdDate |Compliance  |5.487828779742072E-4|2020-12-30 19:53:42.777|orders      |\n",
      "+-------+-----------------------------------+------------+--------------------+-----------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = list(map(Result,map(read,Parser.sections()),Parser.sections()))\n",
    "merged_analyzer = reduce(DataFrame.unionAll, result)\n",
    "merged_analyzer.write.csv(f'..//analyzer_files//merged_analyzer_{calendar.timegm(time.gmtime())}',header=True,mode='overwrite')\n",
    "merged_analyzer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
