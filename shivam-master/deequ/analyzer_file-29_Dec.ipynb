{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import pydeequ, pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pydeequ import Check,CheckLevel\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.analyzers import *\n",
    "from pyspark.sql.functions import lit,current_timestamp\n",
    "from configparser import ConfigParser\n",
    "import sys\n",
    "import calendar,time\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\config\\\\config_analysis.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parser = ConfigParser()\n",
    "configFile = r\"..\\config\\config_analysis.json\"\n",
    "Parser.read(configFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"C:\\Program Files (x86)\\PostgreSQL\\pgJDBC\\postgresql-42.2.18.jar\") \\\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchants = spark.read.format('csv').load(Parser.get('merchants','raw_s3_path'),header=True,inferSchema=True).select('id','state','updated_at','telephone')\n",
    "df_users = spark.read.format('csv').load(Parser.get('users','raw_s3_path'),header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Result(df, source_table):\n",
    "    analyzers = Parser.get(source_table,'metrics').split(',')\n",
    "    analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(eval(\"Size()\"))\n",
    "\n",
    "    res = lambda fun : analysisResult.addAnalyzer(eval(fun))\n",
    "    tsp \n",
    "    analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, list(map(res,analyzers))[0].run())\n",
    "    analysisResult_df = analysisResult_df.withColumn('ingestion_tsp',lit(current_timestamp()))\n",
    "    analysisResult_df = analysisResult_df.withColumn('source_table',lit(source_table))\n",
    "    analysisResult_df.write.csv(f'..//analyzer_files//{source_table}_{calendar.timegm(time.gmtime())}',header=True,mode='overwrite')\n",
    "\n",
    "    return analysisResult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "| entity|     instance|        name|    value|       ingestion_tsp|source_table|\n",
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "|Dataset|            *|        Size|   1000.0|2020-12-29 19:18:...|   merchants|\n",
      "| Column|           id|Distinctness|      1.0|2020-12-29 19:18:...|   merchants|\n",
      "| Column|           id|Completeness|      1.0|2020-12-29 19:18:...|   merchants|\n",
      "| Column|           id|     Minimum|    469.0|2020-12-29 19:18:...|   merchants|\n",
      "| Column|           id|     Maximum| 174577.0|2020-12-29 19:18:...|   merchants|\n",
      "| Column|    telephone|Completeness|    0.999|2020-12-29 19:18:...|   merchants|\n",
      "| Column|sign_in_count|     Minimum|      0.0|2020-12-29 19:18:...|       users|\n",
      "| Column|sign_in_count|     Maximum|4187003.0|2020-12-29 19:18:...|       users|\n",
      "| Column|        email|Completeness|      1.0|2020-12-29 19:18:...|       users|\n",
      "|Dataset|            *|        Size| 154817.0|2020-12-29 19:18:...|       users|\n",
      "| Column|           id|Completeness|      1.0|2020-12-29 19:18:...|       users|\n",
      "+-------+-------------+------------+---------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = list(map(Result,[df_merchants,df_users],Parser.sections()))\n",
    "merged_analyzer = reduce(DataFrame.unionAll, result)\n",
    "merged_analyzer.write.csv(f'..//analyzer_files//merged_analyzer_{calendar.timegm(time.gmtime())}',header=True,mode='overwrite')\n",
    "merged_analyzer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
