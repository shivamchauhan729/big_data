{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbaaadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import HiveContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f53c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/11 13:38:57 WARN Utils: Your hostname, shivam-Vostro-3559 resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlx7cc2c61989aa)\n",
      "21/11/11 13:38:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/11/11 13:39:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ba7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7620278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id=local-1636618147332\n",
      "spark.app.name=pyspark-shell\n",
      "spark.app.startTime=1636618142387\n",
      "spark.driver.host=192.168.0.105\n",
      "spark.driver.port=44475\n",
      "spark.executor.id=driver\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.sql.catalogImplementation=hive\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n"
     ]
    }
   ],
   "source": [
    "conf_out = sc.getConf()\n",
    "print(conf_out.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d967b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caafe07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 11:39:43 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 11:39:43 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 11:39:52 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "21/11/07 11:39:52 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore shivam@127.0.1.1\n",
      "21/11/07 11:39:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"CREATE DATABASE IF NOT EXISTS OFFICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2c4128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"drop table if exists office.employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b8c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 11:39:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/11/07 11:39:56 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 11:39:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 11:39:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 11:39:56 WARN HiveMetaStore: Location: file:/home/shivam/notebook/spark-warehouse/office.db/employee specified for non-external table:employee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"CREATE TABLE IF NOT EXISTS OFFICE.EMPLOYEE(id INT, name STRING, age STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ec7124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"LOAD DATA LOCAL INPATH 'data/employee.txt' INTO TABLE OFFICE.EMPLOYEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e403f3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "|1203|   amith| 39|\n",
      "|1204|   javed| 23|\n",
      "|1205|  prudvi| 33|\n",
      "|1206|  drudvi| 26|\n",
      "|1202|   krudi| 20|\n",
      "|1203|  drudvi| 27|\n",
      "|1202|  nrudvi| 29|\n",
      "|1201|  mrudvi| 35|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from OFFICE.EMPLOYEE\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3451c10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>   (187 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----------+\n",
      "|  id|       average_age|num_persons|\n",
      "+----+------------------+-----------+\n",
      "|1206|              26.0|          1|\n",
      "|1205|              33.0|          1|\n",
      "|1204|              23.0|          1|\n",
      "|1203|              33.0|          2|\n",
      "|1202|25.666666666666668|          3|\n",
      "|1201|              30.0|          2|\n",
      "+----+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"create view if not exists office.v_emp as \n",
    "        select id, avg(age) as average_age, count(id) as num_persons\n",
    "        from office.employee \n",
    "        group by id\n",
    "        order by id desc\"\"\");\n",
    "hc.sql(\"\"\"select * from office.v_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae797ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   null|\n",
      "|    name|   string|   null|\n",
      "|     age|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a47db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|id                          |int                                                          |null   |\n",
      "|name                        |string                                                       |null   |\n",
      "|age                         |string                                                       |null   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Database                    |office                                                       |       |\n",
      "|Table                       |employee                                                     |       |\n",
      "|Owner                       |shivam                                                       |       |\n",
      "|Created Time                |Sun Nov 07 11:39:56 IST 2021                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.1.2                                                  |       |\n",
      "|Type                        |MANAGED                                                      |       |\n",
      "|Provider                    |hive                                                         |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636265397]                           |       |\n",
      "|Statistics                  |168 bytes                                                    |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/employee|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe           |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                     |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat   |       |\n",
      "|Storage Properties          |[serialization.format=,, line.delim=\n",
      ", field.delim=,]        |       |\n",
      "|Partition Provider          |Catalog                                                      |       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.employee\"\"\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a778f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 12:52:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "21/11/07 12:52:55 WARN HiveMetaStore: Location: file:/home/shivam/notebook/spark-warehouse/office.db/emp_copy specified for non-external table:emp_copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"create table IF NOT EXISTS office.emp_copy as select * from office.employee\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8334c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "|1203|   amith| 39|\n",
      "|1204|   javed| 23|\n",
      "|1205|  prudvi| 33|\n",
      "|1206|  drudvi| 26|\n",
      "|1202|   krudi| 20|\n",
      "|1203|  drudvi| 27|\n",
      "|1202|  nrudvi| 29|\n",
      "|1201|  mrudvi| 35|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.emp_copy\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2636b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|id                          |int                                                          |null   |\n",
      "|name                        |string                                                       |null   |\n",
      "|age                         |string                                                       |null   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Database                    |office                                                       |       |\n",
      "|Table                       |emp_copy                                                     |       |\n",
      "|Owner                       |shivam                                                       |       |\n",
      "|Created Time                |Sun Nov 07 12:52:55 IST 2021                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.1.2                                                  |       |\n",
      "|Type                        |MANAGED                                                      |       |\n",
      "|Provider                    |hive                                                         |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636269775]                           |       |\n",
      "|Statistics                  |168 bytes                                                    |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/emp_copy|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe           |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                     |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat   |       |\n",
      "|Storage Properties          |[serialization.format=1]                                     |       |\n",
      "|Partition Provider          |Catalog                                                      |       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.emp_copy\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca511c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert into table office.emp_copy \n",
    "        select * from office.employee\"\"\").show()\n",
    "\n",
    "## this will append the contents of employee table to emp_copy table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5ab04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      20|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select count(*) from office.emp_copy\"\"\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "861a8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select count(*) from office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af39f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite table office.emp_copy\n",
    "        select * from office.employee\"\"\")\n",
    "\n",
    "## this will overwrite the content of table emp_copy by employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed661295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select count(*) from office.emp_copy\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06add501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1202| krishna| 28|\n",
      "|1202|   krudi| 20|\n",
      "|1202|  nrudvi| 29|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.emp_copy where id = 1202\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d876b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "|1202|   krudi| 20|\n",
      "|1202|  nrudvi| 29|\n",
      "|1201|  mrudvi| 35|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.emp_copy where id in (1202,1201)\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc3cfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.employee where name rlike 's' \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fa9d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+------------+\n",
      "|  id|    name|age|age_category|\n",
      "+----+--------+---+------------+\n",
      "|1201|  satish| 25|     teenage|\n",
      "|1202| krishna| 28|       adult|\n",
      "|1203|   amith| 39|     invalid|\n",
      "|1204|   javed| 23|       child|\n",
      "|1205|  prudvi| 33|         old|\n",
      "|1206|  drudvi| 26|     teenage|\n",
      "|1202|   krudi| 20|       child|\n",
      "|1203|  drudvi| 27|     teenage|\n",
      "|1202|  nrudvi| 29|       adult|\n",
      "|1201|  mrudvi| 35|         old|\n",
      "+----+--------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select *, \n",
    "        CASE WHEN age >= 20 and age < 24 THEN 'child'\n",
    "            WHEN age >=24 and age < 28 THEN 'teenage'\n",
    "            WHEN age >= 28 and age < 32 THEN 'adult'\n",
    "            WHEN age >= 32 and age<36 then 'old'\n",
    "            ELSE 'invalid'\n",
    "        END AS age_category\n",
    "        from office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bca6899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "|1201|\n",
      "|1206|\n",
      "|1205|\n",
      "|1203|\n",
      "|1202|\n",
      "|1204|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select distinct id from office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5853c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|  id|avg(CAST(age AS DOUBLE))|\n",
      "+----+------------------------+\n",
      "|1201|                    30.0|\n",
      "|1206|                    26.0|\n",
      "|1205|                    33.0|\n",
      "|1203|                    33.0|\n",
      "|1202|      25.666666666666668|\n",
      "|1204|                    23.0|\n",
      "+----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select id, avg(age) from office.employee group by id \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a361c329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|  id|avg(CAST(age AS DOUBLE))|\n",
      "+----+------------------------+\n",
      "|1201|                    30.0|\n",
      "|1205|                    33.0|\n",
      "|1203|                    33.0|\n",
      "+----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select id, avg(age) from office.employee group by id having avg(age) > 26\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "989e46b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creates a file in the required location for a temporary query, for the future usage\n",
    "hc.sql(\"\"\"insert overwrite local directory 'testing_data' \n",
    "        row format delimited fields terminated by ',' \n",
    "        select id, avg(age) from office.employee group by id having avg(age)>26\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d38a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|id                          |int                                                          |null   |\n",
      "|name                        |string                                                       |null   |\n",
      "|age                         |string                                                       |null   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Database                    |office                                                       |       |\n",
      "|Table                       |emp_copy                                                     |       |\n",
      "|Owner                       |shivam                                                       |       |\n",
      "|Created Time                |Sun Nov 07 12:52:55 IST 2021                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.1.2                                                  |       |\n",
      "|Type                        |MANAGED                                                      |       |\n",
      "|Provider                    |hive                                                         |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636269780]                           |       |\n",
      "|Statistics                  |168 bytes                                                    |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/emp_copy|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe           |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                     |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat   |       |\n",
      "|Storage Properties          |[serialization.format=1]                                     |       |\n",
      "|Partition Provider          |Catalog                                                      |       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.emp_copy\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b738da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"drop table office.emp_copy\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc87c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls spark-warehouse/office.db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d5bf393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS OFFICE.emp_copy(\n",
    "        id INT, name STRING, age STRING\n",
    "        ) \n",
    "        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\n",
    "        location 'emp_copy'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d30637b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_copy\n",
      "employee\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls spark-warehouse/office.db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cb2ce78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|id                          |int                                                          |null   |\n",
      "|name                        |string                                                       |null   |\n",
      "|age                         |string                                                       |null   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Database                    |office                                                       |       |\n",
      "|Table                       |emp_copy                                                     |       |\n",
      "|Owner                       |shivam                                                       |       |\n",
      "|Created Time                |Sun Nov 07 12:53:09 IST 2021                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.1.2                                                  |       |\n",
      "|Type                        |EXTERNAL                                                     |       |\n",
      "|Provider                    |hive                                                         |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636269789]                           |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/emp_copy|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe           |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                     |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat   |       |\n",
      "|Storage Properties          |[serialization.format=,, line.delim=\n",
      ", field.delim=,]        |       |\n",
      "|Partition Provider          |Catalog                                                      |       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.emp_copy\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbc9a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select  * from office.emp_copy\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "174d3999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"load data local inpath 'spark-warehouse/office.db/employee/employee.txt'\n",
    "         into table office.emp_copy\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cd9d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "|1203|   amith| 39|\n",
      "|1204|   javed| 23|\n",
      "|1205|  prudvi| 33|\n",
      "|1206|  drudvi| 26|\n",
      "|1202|   krudi| 20|\n",
      "|1203|  drudvi| 27|\n",
      "|1202|  nrudvi| 29|\n",
      "|1201|  mrudvi| 35|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.emp_copy\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9d81b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls spark-warehouse/office.db/emp_copy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ced1e486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now if we drop the external table emp_copy, lets see the dataset if it gets dropped as well\n",
    "\n",
    "hc.sql(\"\"\"drop table office.emp_copy\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7fb8d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls spark-warehouse/office.db/emp_copy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "427f91ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: office.emp_copy; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [office, emp_copy], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5834/3363008438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## you can see the dataset doesnt get deleted, though the table and its structure gets dropped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"select * from office.emp_copy\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: office.emp_copy; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [office, emp_copy], [], false\n"
     ]
    }
   ],
   "source": [
    "## you can see the dataset doesnt get deleted, though the table and its structure gets dropped\n",
    "\n",
    "hc.sql(\"\"\"select * from office.emp_copy\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a373086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"create external table if not exists office.emp_copy_2 (id int, age string, name string)\n",
    "        row format delimited fields terminated by ','\n",
    "        location '../testing_external_table/emp_copy_2'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b41f70e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+\n",
      "|  id|     age|name|\n",
      "+----+--------+----+\n",
      "|1201|  satish|  25|\n",
      "|1202| krishna|  28|\n",
      "|1203|   amith|  39|\n",
      "|1204|   javed|  23|\n",
      "|1205|  prudvi|  33|\n",
      "|1206|  drudvi|  26|\n",
      "|1202|   krudi|  20|\n",
      "|1203|  drudvi|  27|\n",
      "|1202|  nrudvi|  29|\n",
      "|1201|  mrudvi|  35|\n",
      "+----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## now i have manually moved files to the external_table location, now it works\n",
    "\n",
    "hc.sql(\"\"\"select * from office.emp_copy_2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f49db1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                   |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                         |null   |\n",
      "|age                         |string                                                                      |null   |\n",
      "|name                        |string                                                                      |null   |\n",
      "|                            |                                                                            |       |\n",
      "|# Detailed Table Information|                                                                            |       |\n",
      "|Database                    |office                                                                      |       |\n",
      "|Table                       |emp_copy_2                                                                  |       |\n",
      "|Owner                       |shivam                                                                      |       |\n",
      "|Created Time                |Wed Nov 03 15:40:16 IST 2021                                                |       |\n",
      "|Last Access                 |UNKNOWN                                                                     |       |\n",
      "|Created By                  |Spark 3.1.2                                                                 |       |\n",
      "|Type                        |EXTERNAL                                                                    |       |\n",
      "|Provider                    |hive                                                                        |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1635934216]                                          |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/testing_external_table/emp_copy_2|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                          |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                    |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                  |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                                     |       |\n",
      "|Partition Provider          |Catalog                                                                     |       |\n",
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.emp_copy_2\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2be30ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite local directory 'testing_data/' row format delimited fields terminated by ','\n",
    "        select * from office.employee sort by id desc\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2edbefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"set mapreduce.job.reduce = 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "904fec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite local directory 'testing_data/test' row format delimited fields terminated by ','\n",
    "        select * from office.employee sort by id desc\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c81676d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite local directory 'testing_data/test_distribute' row format delimited \n",
    "        fields terminated by ',' \n",
    "        select * from office.employee \n",
    "        distribute by id \n",
    "        sort by id desc\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd1643c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is similar to distribute by + sort by, the only need is we need to use same set of columns in \n",
    "# distrubute by and sort by in order to have same results \n",
    "hc.sql(\"\"\"insert overwrite local directory 'testing_data/test_distribute/' row format delimited \n",
    "        fields terminated by ','\n",
    "        select * from office.employee\n",
    "        cluster by id\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03033e48",
   "metadata": {},
   "source": [
    "### partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "added47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 13:42:56 WARN HiveMetaStore: Location: file:/home/shivam/notebook/spark-warehouse/office.db/partition_emp specified for non-external table:partition_emp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"create table if not exists office.partition_emp (name STRING, age STRING)\n",
    "        partitioned by (id INT)\n",
    "        row format delimited fields terminated by ','\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c63d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|name                        |string                                                            |null   |\n",
      "|age                         |string                                                            |null   |\n",
      "|id                          |int                                                               |null   |\n",
      "|# Partition Information     |                                                                  |       |\n",
      "|# col_name                  |data_type                                                         |comment|\n",
      "|id                          |int                                                               |null   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Database                    |office                                                            |       |\n",
      "|Table                       |partition_emp                                                     |       |\n",
      "|Owner                       |shivam                                                            |       |\n",
      "|Created Time                |Sun Nov 07 13:42:56 IST 2021                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.1.2                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636272776]                                |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/partition_emp|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                          |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat        |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                           |       |\n",
      "|Partition Provider          |Catalog                                                           |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.partition_emp\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5d461d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inserting specific partition from target table\n",
    "hc.sql(\"\"\"insert overwrite table office.partition_emp\n",
    "        partition (id = 1201)\n",
    "        select name,age from office.employee\n",
    "        where id = 1201\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10071398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|   name|age|  id|\n",
      "+-------+---+----+\n",
      "| satish| 25|1201|\n",
      "| mrudvi| 35|1201|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2899070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite table office.partition_emp\n",
    "        partition (id = 1202)\n",
    "        select name,age from office.employee\n",
    "        where id=1202\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17896880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+\n",
      "|    name|age|  id|\n",
      "+--------+---+----+\n",
      "|  satish| 25|1201|\n",
      "|  mrudvi| 35|1201|\n",
      "| krishna| 28|1202|\n",
      "|   krudi| 20|1202|\n",
      "|  nrudvi| 29|1202|\n",
      "+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"select * from office.partition_emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d904e1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  id=1201|\n",
      "|  id=1202|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"show partitions office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "425b9caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|name                        |string                                                            |null   |\n",
      "|age                         |string                                                            |null   |\n",
      "|id                          |int                                                               |null   |\n",
      "|# Partition Information     |                                                                  |       |\n",
      "|# col_name                  |data_type                                                         |comment|\n",
      "|id                          |int                                                               |null   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Database                    |office                                                            |       |\n",
      "|Table                       |partition_emp                                                     |       |\n",
      "|Owner                       |shivam                                                            |       |\n",
      "|Created Time                |Sun Nov 07 13:42:56 IST 2021                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.1.2                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1636272776]                                |       |\n",
      "|Location                    |file:/home/shivam/notebook/spark-warehouse/office.db/partition_emp|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                          |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat        |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                           |       |\n",
      "|Partition Provider          |Catalog                                                           |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"describe formatted office.partition_emp\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51496383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1201\n",
      "id=1202\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls /home/shivam/notebook/spark-warehouse/office.db/partition_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f623652a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite local directory '/home/shivam/notebook/spark-warehouse/office.db/partition_emp/id=1203'\n",
    "        row format delimited fields terminated by ','\n",
    "        select name,age from office.employee where id=1203\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c71636fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+\n",
      "|    name|age|  id|\n",
      "+--------+---+----+\n",
      "|  satish| 25|1201|\n",
      "|  mrudvi| 35|1201|\n",
      "| krishna| 28|1202|\n",
      "|   krudi| 20|1202|\n",
      "|  nrudvi| 29|1202|\n",
      "+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14b6507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1201\n",
      "id=1202\n",
      "id=1203\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls /home/shivam/notebook/spark-warehouse/office.db/partition_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d79f1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name|age| id|\n",
      "+----+---+---+\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## this will just create a directory with different name ,,, thats it, the partition is not added yet\n",
    "hc.sql(\"\"\"select * from office.partition_emp where id=1203\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7094b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 14:16:41 WARN log: Updating partition stats fast for: partition_emp\n",
      "21/11/07 14:16:41 WARN log: Updated size to 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we need to add partition to the table using alter cmd\n",
    "hc.sql(\"\"\"alter table office.partition_emp add if not exists \n",
    "            partition (id=1203) location '/home/shivam/notebook/spark-warehouse/office.db/partition_emp/id=1203'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30b9be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+\n",
      "|    name|age|  id|\n",
      "+--------+---+----+\n",
      "|  satish| 25|1201|\n",
      "|  mrudvi| 35|1201|\n",
      "| krishna| 28|1202|\n",
      "|   krudi| 20|1202|\n",
      "|  nrudvi| 29|1202|\n",
      "|   amith| 39|1203|\n",
      "|  drudvi| 27|1203|\n",
      "+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## now the partition is added and we can now show the partitions in the table\n",
    "hc.sql(\"\"\"select * from office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bf68ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  id=1201|\n",
      "|  id=1202|\n",
      "|  id=1203|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"show partitions office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aedc94d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"alter table office.partition_emp drop if exists partition (id=1203)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "334dc272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  id=1201|\n",
      "|  id=1202|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"show partitions office.partition_emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd1f26af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first config is deprecated, when using hive in spark , prefix spark in the config\n",
    "# hc.sql(\"\"\"set hive.exec.dynamic.partition=true\"\"\")\n",
    "hc.sql(\"\"\"set spark.hadoop.hive.exec.dynamic.partition=true\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e37f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"set spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac659401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/07 14:36:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"insert overwrite table office.partition_emp\n",
    "        partition (id)\n",
    "        select name,age,id from office.employee\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97224f51",
   "metadata": {},
   "source": [
    "## configure the dynamic partitions\n",
    "hc.sql(\"\"\"set spark.hadoop.hive.exec.max.dynamic.partitions=1000\"\"\")\n",
    "hc.sql(\"\"\"set spark.hadoop.hive.exec.max.dynamic.partitions.pernode=500\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e283a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|   name|age|  id|\n",
      "+-------+---+----+\n",
      "| satish| 25|1201|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.partition_emp where age=25\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6533d443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|  id|    name|age|\n",
      "+----+--------+---+\n",
      "|1201|  satish| 25|\n",
      "|1202| krishna| 28|\n",
      "|1203|   amith| 39|\n",
      "|1204|   javed| 23|\n",
      "|1205|  prudvi| 33|\n",
      "|1206|  drudvi| 26|\n",
      "|1202|   krudi| 20|\n",
      "|1203|  drudvi| 27|\n",
      "|1202|  nrudvi| 29|\n",
      "|1201|  mrudvi| 35|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34e5a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+\n",
      "|  id|    name|       running_avg|\n",
      "+----+--------+------------------+\n",
      "|1201|  satish|              30.0|\n",
      "|1201|  mrudvi|              30.0|\n",
      "|1206|  drudvi|              26.0|\n",
      "|1205|  prudvi|              33.0|\n",
      "|1203|   amith|              33.0|\n",
      "|1203|  drudvi|              33.0|\n",
      "|1202| krishna|25.666666666666668|\n",
      "|1202|   krudi|25.666666666666668|\n",
      "|1202|  nrudvi|25.666666666666668|\n",
      "|1204|   javed|              23.0|\n",
      "+----+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select id,name,\n",
    "                avg(age) over(PARTITION BY id ) \n",
    "                as running_avg\n",
    "        from office.employee\"\"\").show()\n",
    "\n",
    "## first the dataset is partitioned or grouped by age , this is known as window\n",
    "## and next analytical function gets executed for  each record\n",
    "## the average is still calculated for each record in the partition or window but it is executed for each record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96653e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ac41590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+\n",
      "|  id|    name|       running_avg|\n",
      "+----+--------+------------------+\n",
      "|1201|  satish|              25.0|\n",
      "|1201|  mrudvi|              30.0|\n",
      "|1206|  drudvi|              26.0|\n",
      "|1205|  prudvi|              33.0|\n",
      "|1203|   amith|              39.0|\n",
      "|1203|  drudvi|              33.0|\n",
      "|1202| krishna|              28.0|\n",
      "|1202|   krudi|              24.0|\n",
      "|1202|  nrudvi|25.666666666666668|\n",
      "|1204|   javed|              23.0|\n",
      "+----+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select id,name,\n",
    "                avg(age) over(PARTITION BY id rows between unbounded preceding and current row) \n",
    "                as running_avg\n",
    "        from office.employee\"\"\").show()\n",
    "\n",
    "## first the dataset is partitioned or grouped by age , this is known as window\n",
    "## and then frames are generated that would include the current row and the rows behind it\n",
    "## next analytical function gets executed for  each frame generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc.sql(\"\"\"select ymd, exchange, symbol, volume, price_close,\n",
    "            avg(price_close) over(PARTITION BY symbol ORDER BY ymd\n",
    "                                ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS 10_DAY_MOVING_AVG\n",
    "        from stocks\n",
    "        where symbol in ('abc','def')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a7f5fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/08 00:12:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "21/11/08 00:12:32 WARN HiveMetaStore: Location: file:/home/shivam/notebook/spark-warehouse/office.db/emp_old_age specified for non-external table:emp_old_age\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"\"\"create table if not exists office.emp_old_age as\n",
    "        select id, name, age from (\n",
    "        select * ,\n",
    "            row_number() over (partition by id order by age desc) as row_number\n",
    "        from office.employee\n",
    "        )\n",
    "        \n",
    "        where row_number=1\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e0d38168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n",
      "|  id|   name|age|\n",
      "+----+-------+---+\n",
      "|1201| mrudvi| 35|\n",
      "|1203|  amith| 39|\n",
      "|1202| nrudvi| 29|\n",
      "|1204|  javed| 23|\n",
      "|1206| drudvi| 26|\n",
      "|1205| prudvi| 33|\n",
      "+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"\"\"select * from office.emp_old_age\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f2735d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----+------------+---------+\n",
      "|  id|    name|age|rank|previous_age|after_age|\n",
      "+----+--------+---+----+------------+---------+\n",
      "|1201|  satish| 25|   1|        null|       35|\n",
      "|1201|  mrudvi| 35|   2|          25|     null|\n",
      "|1206|  drudvi| 26|   1|        null|     null|\n",
      "|1205|  prudvi| 33|   1|        null|     null|\n",
      "|1203|  drudvi| 27|   1|        null|       39|\n",
      "|1203|   amith| 39|   2|          27|     null|\n",
      "|1202|   krudi| 20|   1|        null|       28|\n",
      "|1202| krishna| 28|   2|          20|       29|\n",
      "|1202|  nrudvi| 29|   3|          28|     null|\n",
      "|1204|   javed| 23|   1|        null|     null|\n",
      "+----+--------+---+----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## lead checks for the value in the attribute in the next record of  the window,\n",
    "## lag checks for the value in the attribute in the previous record of the window.\n",
    "hc.sql(\"\"\"select *,\n",
    "        row_number() over(partition by id order by age) as rank,\n",
    "        lag(age,1) over(partition by id order by age) as previous_age,\n",
    "        lead(age,1) over(partition by id order by age) as after_age\n",
    "    from office.employee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbfac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
